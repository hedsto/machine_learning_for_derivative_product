{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3tJbeFLuoHNv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from scipy.optimize import least_squares\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxeYB-BfoHNx",
        "outputId": "242ef8bd-ec09-410c-a927-6181ee0fd105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed =  261872297657270704497041869730793872727\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.13526605, -0.6145526 ,  0.22542003, -0.92730968, -1.14693194])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from numpy.random import default_rng, SeedSequence\n",
        "\n",
        "sq = SeedSequence()\n",
        "seed = sq.entropy        # on sauve la graine pour reproduire les résultats\n",
        "print('seed = ', seed)\n",
        "rng = default_rng(sq)\n",
        "rng.standard_normal(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jhx-FBADoHNy"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "mpl.rcParams['figure.dpi'] = 100\n",
        "\n",
        "params_grid = {\"color\": 'lightgrey', \"linestyle\": 'dotted', \"linewidth\": 0.7 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CKFPiEnkoHNz"
      },
      "outputs": [],
      "source": [
        "# Pamètre du model\n",
        "N = 10\n",
        "M = 10000\n",
        "\n",
        "T = 1\n",
        "dt = T/N\n",
        "S0 = 100\n",
        "K = 100\n",
        "sigma = 0.1\n",
        "r = 0.04\n",
        "m = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkrYgwzs4j3d"
      },
      "source": [
        "We are going to compare the result of the LS algorithm when using a linear regression and when using a Neural Network (Non linear regression) to learn the continuation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSIMJaQA36mD"
      },
      "source": [
        "We are going to code the Longstaff-Schwartz algorithm for an American Put with respect to a Black Scholes model, that is to say a put that we can exerce at $t_k = \\frac{k}{N}$ where $k = 0,..., N$. We will take $r=0.04$, $\\sigma=0.1$, $x_0=100$ and the strike $K=100$ with $N=10$ dates until $T=1$ and the payoff $\\phi_k(x) = e^{-r k/N}(K-x)_+$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U0bp9vO4nGS"
      },
      "source": [
        "For that we are going to reuse the notation of the 2nd project :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p1ckDhXyoHN0"
      },
      "outputs": [],
      "source": [
        "def brownian_1d(n_times: int, n_paths: int,\n",
        "                final_time: float=1.0,\n",
        "                increments: bool=False,\n",
        "                random_state: np.random.Generator=rng) -> np.array:\n",
        "    \"\"\"Simulate paths of standard Brownian motion\n",
        "    Args:\n",
        "        n_times: Number of timesteps\n",
        "        n_paths: Number of paths\n",
        "        final_time: Final time of simulation\n",
        "        increments: If `True` the increments of the paths are returned.\n",
        "        random_state: `np.random.Generator` used for simulation\n",
        "    Returns:\n",
        "        `np.array` of shape `(n_times+1, n_paths)` containing the paths if the argument `increments` is `False`\n",
        "        `np.array` of shape `(n_times, n_paths)` containing the increments if the argument `increments` is `True`\n",
        "    \"\"\"\n",
        "    dB = np.sqrt(final_time / n_times) * random_state.standard_normal((n_times, n_paths))\n",
        "    if increments:\n",
        "        return dB\n",
        "    else:\n",
        "        brownian = np.zeros((n_times+1, n_paths))\n",
        "        brownian[1:] = np.cumsum(dB, axis=0)\n",
        "        return brownian\n",
        "\n",
        "def black_scholes_1d(n_times: int, n_paths: int,\n",
        "                     final_time: float=1.0,\n",
        "                     random_state: np.random.Generator=rng, *,\n",
        "                     init_value: float,\n",
        "                     r: float, sigma: float) -> np.array:\n",
        "    \"\"\"Simulate paths of Black-Scholes process\n",
        "    Args:\n",
        "        n_times: Number of timesteps\n",
        "        n_paths: Number of paths\n",
        "        final_time: Final time of simulation\n",
        "        init_value: `S0`\n",
        "        r: Interest rate\n",
        "        sigma: Volatility\n",
        "        random_state: `np.random.Generator` used for simulation\n",
        "    Returns:\n",
        "        `np.array` of shape `(n_times+1, n_paths)` containing the paths\n",
        "    \"\"\"\n",
        "    Bt = brownian_1d(n_times, n_paths)\n",
        "    times = np.arange(n_times+1)*(1/n_times)\n",
        "    t = times[:, np.newaxis]\n",
        "    St = init_value * np.exp((r - 0.5*sigma**2)*t + sigma*Bt)\n",
        "    return St"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ry4OXcT5BlM"
      },
      "source": [
        "To find the value $V_0$, we apply the following backward recurrence :\n",
        "$V_N(x) = \\phi_N(x)$\n",
        "\n",
        "and $V_n(x) = max(\\phi_n(x), \\mathbb{E}[V_{n+1}(x))$\n",
        "\n",
        "The aim of the LS algorithm is to approch the conditional expectation with function.\n",
        "\n",
        "For that we use the following property :\n",
        "$$\\theta^* = \\arg \\min_{\\theta} \\mathbb{E}[(\\mathbb{E}(Y|X) - \\Phi(X, \\theta))^2]$$\n",
        "$$ = \\arg \\min_{\\theta} \\mathbb{E}[(Y - \\Phi(X, \\theta))^2]$$\n",
        "In the following example we will use Linear Regression, the algorithm become :\n",
        "$$V_N(x) = \\phi_N(x)$$\n",
        "\n",
        "$$V_n(x) = max(\\phi_n(x), \\Phi(x,\\theta_n))$$\n",
        "\n",
        "with $\\theta_n = argmin_\\theta \\sum_{j=1}^{M} \\left( V_{n+1}(X_{n+1}^{(j)}) - \\Phi(X_{n}^{(j)}, \\theta) \\right)^2$\n",
        "\n",
        "For $\\Phi$ we can choose between two families of function :\n",
        "\n",
        "$\\Phi(x) = \\theta_0+\\theta_1*x+\\theta_2*x^2$\n",
        "\n",
        "or\n",
        "\n",
        " $\\Phi(x) = \\theta_0+\\theta_1*x+\\theta_2*x^2+\\theta_3*max(K-x, 0)$\n",
        "\n",
        " In the following, I used the first version $\\Phi$ but we will found the result for the first version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5AZG8oMYoHN0"
      },
      "outputs": [],
      "source": [
        "# psi est ma régression linéaire qui va me permettre d'approcher le résultat théorique\n",
        "# Le but étant de trouver les theta optimaux\n",
        "def psi(x, theta, m):\n",
        "    res = 0\n",
        "    for i in range(m):\n",
        "      res += theta[i]*(x**i)\n",
        "    return res\n",
        "\n",
        "def phi(x, K, k, N):\n",
        "    return np.exp(-r*k/N)*np.maximum(K-x, 0)\n",
        "\n",
        "#same function as the previous one but it allows to manipulate matrix\n",
        "#it will be useful when I will use a regression to find theta based on the prices obtained by Black Scholes simulation\n",
        "def phi_regression(x, K, k, N):\n",
        "    return np.exp(-r*k/N)*np.maximum(K-x[k], 0)\n",
        "\n",
        "#Function where we want to find the argmin theta\n",
        "def objective_function(theta, M, S, n, K, thetas):\n",
        "    result = 0\n",
        "    for j in range(M):\n",
        "        result += (V(S[n+1][j], n+1, thetas[-1], N, K) - psi(S[n][j], theta, K))**2\n",
        "    return result/M\n",
        "\n",
        "def V(x, n, theta,N, K):\n",
        "    if n == N:\n",
        "        return phi(x,K,N,N)\n",
        "    else:\n",
        "        return max(phi(x, K,n,N), psi(x, theta, K))\n",
        "\n",
        "def simulate_V(S,K,r,n,V):\n",
        "    if n==-1:\n",
        "        return V\n",
        "    else :\n",
        "        V[n] = np.maximum(phi_regression(S,K,n,N),V[n+1])\n",
        "        return simulate_V(S,K,r,n-1,V)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aawFlCvgoHN1"
      },
      "source": [
        "# Régression Linéaire"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let $\\left( e_{k}(x) \\right)_{k \\leq L}$ a function family $e_{k} : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$.\n",
        "\n",
        "We assume $\\forall n, \\, n = 1, \\dots, N-1 \\times (e_{k}(X_n))_{k \\leq L}$, is a total generative family of $L^{2}(\\sigma(X_n))$\n",
        "\n",
        "Let $m$ finite, (linearly independant family, ie $(e_k(X_n))_{1 \\leq k \\leq m}$ is a base).\n",
        "\n",
        "We take the orthogonal projection on  $\\text{Vect}(e_1(X_n), \\dots, e_m(X_n))$, ie :\n",
        "\n",
        "\n",
        "$$\\Phi_{m}(x ; \\theta) = \\sum_{k=1}^{m} \\theta_k e_k(x), \\quad \\theta = (\\theta_1, \\dots, \\theta_m)^T$$\n",
        "\n",
        "The optimal stopping time is given by :\n",
        "\n",
        "$$τ_N^m = M$$\n",
        "$$τ_n^m = n1_{ϕ_{n}(X_n)>\\Phi_{m}(x ; \\theta_n^m)} + τ_{n+1}^m1_{ϕ_{n}(X_n)<\\Phi_{m}(x ; \\theta_n^m)}$$\n",
        "\n",
        "\n",
        "**Important** : At each step $n$, we need to determine $\\theta$ such that :\n",
        "\n",
        "For simplicity we denote : $Z_{n+1}^{(m)} = Z_{τ_{n+1}^m}$\n",
        "\n",
        "$$\\arg \\min_{\\theta \\in \\mathbb{R}^m} E \\left[ \\left( Z_n^{(m)} - \\sum_{k=1}^{m} \\theta_k e_k(X_n) \\right)^2 \\right]$$\n",
        "\n",
        "We minimize thee funciton $W(\\theta)$ defined by :\n",
        "\n",
        "\n",
        "$$W(\\theta) = E \\left[ \\left( Z_n^{(m)} - \\sum_{k=1}^{m} \\theta_k e_k(X_n) \\right)^2 \\right]$$\n",
        "\n",
        "To find a minimum of W  $W: \\mathbb{R}^m \\rightarrow \\mathbb{R}$, we can search a point $\\theta^*$ that cancels the gradient.\n",
        "\n",
        "The gradient of $W$ is given by :\n",
        "\n",
        "\n",
        "$$\\nabla W(\\theta) : \\mathbb{R}^m \\rightarrow \\mathbb{R}^m$$\n",
        "\n",
        "\n",
        "$$\\nabla W(\\theta) = E \\left[ 2 \\left( Z_n^{(m)} - \\sum_{k=1}^{m} \\theta_k e_k(X_n) \\right) e_j(X_n) \\right]$$\n",
        "\n",
        "Now, we introduce the following notation :\n",
        "\n",
        "\n",
        "$$\\theta = (\\theta_1^{(m)}, \\dots, \\theta_m^{(m)})^T$$\n",
        "\n",
        "\n",
        "$$Z_n^{(m)} = \\sum_{k=1}^{m} \\theta_k^{(m)} e_k(X_n), \\quad \\text{et} \\quad e^{(m)}(X_n) = (e_1(X_n), \\dots, e_m(X_n))^T$$\n",
        "\n",
        "We then have :\n",
        "\n",
        "\n",
        "$$\\nabla W(\\theta) = -E \\left[ \\left( Z_n^{(m)}(x) - e^{(m)}(X_n)^T \\theta \\right) e^{(m)}(X_n) \\right]$$\n",
        "\n",
        "The gradient is null in one point $\\theta = \\left[ A_n^{(m)} \\right]^{-2} E \\left[ Z_n^{(m)}(X) e^{(m)}(X_n) \\right]$ with :\n",
        "\n",
        "\n",
        "$$A_n^{(m)} = e^{(m)}(X_n) e^{(m)}(X_n)^T$$\n",
        "\n",
        "(If the matrix $A_n^{(m)}$ is inversible)\n",
        "\n",
        "In practice, we use emprical laws from a sample$\\left( X_n^{(i)} \\right)_{i = 0, \\dots, N}$ :\n",
        "\n",
        "\n",
        "$$\\hat{\\theta}_n^{(m, H)} = \\left[ \\hat{A}_n^{(m, H)} \\right]^{-1} \\sum_{i=1}^{M} Z_n^{(i)} e^{(m)}(X_n^{(i)})^T$$\n",
        "\n",
        "With :\n",
        "\n",
        "\n",
        "$$\\hat{A}_n^{(m, H)} = \\frac{1}{M} \\sum_{i=1}^{M} e^{(m)}(X_n^{(i)}) e^{(m)}(X_n^{(i)})^T$$\n",
        "\n"
      ],
      "metadata": {
        "id": "SGY4QqZmpyEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GSd5Ofs0oHN2"
      },
      "outputs": [],
      "source": [
        "def regression_theta(X_t, M, m, n):\n",
        "    e_m_X = []\n",
        "    for j in range(M):\n",
        "        e_m_X_j = [X_t[n][j]**k for k in range(m)]\n",
        "\n",
        "        e_m_X.append(np.array(e_m_X_j).reshape(-1, 1))\n",
        "\n",
        "    A_n_m_M = np.zeros((e_m_X[0].dot(e_m_X[0].T).shape))\n",
        "    for i in range(M):\n",
        "        A_n_m_M += e_m_X[j].dot(e_m_X[j].T)\n",
        "    A_n_m_M = (1/M) * A_n_m_M\n",
        "\n",
        "    e_m_X = np.array(e_m_X)\n",
        "    V_simu = np.zeros_like(X_t)\n",
        "    V_simu[-1] = phi_regression(X_t, K, N, N)\n",
        "    Z_n_m =  np.array([simulate_V(X_t[:,j],K,r,V_simu.shape[0]-2,V_simu) for j in range(M)])\n",
        "\n",
        "    esperance = [0 for _ in range(m)]\n",
        "    for j in range(M-1):\n",
        "        for k in range(m):\n",
        "            esperance[k] += Z_n_m[n+1][Z_n_m.shape[1]-1][j]* e_m_X[j][k][0]\n",
        "    esperance = [esperance[k] / M for k in range(m)]\n",
        "\n",
        "    theta_n_m_M = np.linalg.inv(A_n_m_M+0.00001*np.identity(len(A_n_m_M[0]))).dot(esperance)\n",
        "    return theta_n_m_M\n",
        "\n",
        "def longstaff_Schwartz_regression(x0, K, T, N, M, m):\n",
        "    V_hat = [phi(x0, K,N, N)]\n",
        "    S_t = black_scholes_1d(n_times=N, n_paths=M, final_time= T, init_value=x0, r = r, sigma = sigma)\n",
        "    thetas = []\n",
        "    for n in reversed(range(N)):\n",
        "        print(n)\n",
        "        theta = regression_theta(S_t, M, m, n)\n",
        "        optimized_theta = theta#.x\n",
        "        print(optimized_theta)\n",
        "        V_hat.append(max(phi(x0, K,n,N), psi(x0, optimized_theta, m)))\n",
        "        thetas.append(optimized_theta)\n",
        "    return V_hat, thetas, S_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZLBlEWhoHN2",
        "outputId": "ad442f97-b500-4f65-cf63-8b3d9eaac208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "[  99359.80021206 5140868.31868033  -41996.63850465]\n",
            "8\n",
            "[  80214.73892131 4063619.58484872  -35275.29689062]\n",
            "7\n",
            "[  85327.46326978 4412846.82004853  -37174.64190373]\n",
            "6\n",
            "[  84902.76459599 4435150.40718442  -37033.06587941]\n",
            "5\n",
            "[  62813.74232874 3197093.740574    -28476.20416424]\n",
            "4\n",
            "[  72360.05820395 3786376.30098276  -32347.60880932]\n",
            "3\n",
            "[  52298.62436892 2681423.40069135  -24157.44423601]\n",
            "2\n",
            "[  34967.87012467 1770237.15756574  -16571.71037624]\n",
            "1\n",
            "[  20214.85678819 1014381.40342797   -9771.34587879]\n",
            "0\n",
            "[1.83346099e-07 2.15059961e-06 2.21370562e-04]\n",
            "[0.0, 94219806.62173903, 53689204.31758779, 69623590.43078882, 73269284.68896538, 35010146.157340884, 55233902.063266814, 26620196.333433777, 11341579.864268005, 3744896.41168727, 2.213920860723911]\n"
          ]
        }
      ],
      "source": [
        "V_hat, thetas, S_t = longstaff_Schwartz_regression(S0, K, T, N, M, 3)\n",
        "print(V_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRRJrmUboHN3",
        "outputId": "a03aef2a-9f4f-4486-eaf2-483622cde6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price of an American Put with linear Regression is 2.213920860723911 $\n"
          ]
        }
      ],
      "source": [
        "print(\"The price of an American Put with linear Regression is\", V_hat[-1], \"$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-odDIMEo5oPU"
      },
      "source": [
        "We obtained a result closed to the theorical price (2.6$). This is certainly du to the fact that my regression is an approximatioin of theorical formulas but also to the fact that we approach the expectation by using MC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f25NKAk8tnN"
      },
      "source": [
        "## Réseaux de neurones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO2U5bO9x0yy",
        "outputId": "9a2caa5e-520d-4fa1-ae87-ad30b84a7bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approximation by a MLP (Multilayer Perceptron)\n",
        "FNN (Feedforward Neural Networks Fully Connected)\n",
        "\n",
        "Let $\\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ the function we search to approximate the conditional expectation of the LS algorithm\n",
        "\n",
        "\n",
        "$$x \\mapsto \\Phi(x ; \\Theta)$$\n",
        "\n",
        "This is seen as a composition of functions for each layer $L \\geq 2$:\n",
        "\n",
        "\n",
        "$$\\Phi = A_L \\circ \\sigma \\circ A_{L-1} \\circ \\sigma \\circ \\dots \\circ A_1$$\n",
        "\n",
        "where, for $l = 1, \\dots, L$:\n",
        "\n",
        "\n",
        "$$A_l : \\mathbb{R}^{d_{l+1}} \\rightarrow \\mathbb{R}^{d_l}, \\quad \\text{a linear function} \\quad A_l(x) = W_l x + \\beta_l$$\n",
        "\n",
        "- $W_l \\in \\mathbb{R}^{d_{l+1}, d_l}$\n",
        "- $\\beta_l \\in \\mathbb{R}^{d_l}$\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- $d_1 = d$ is the dimension of the inputs.\n",
        "- Here $d_L = 1$.\n",
        "- All parameters $(W_l, \\beta_l)$ for $l = 1, \\dots, L$ are grouped in the notation $\\Theta$.\n",
        "\n",
        "The function $\\sigma$ is called the activation function $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ applied component-wise to its argument. A typical choice is $\\sigma(x) = \\text{ReLU}(x)$.\n",
        "\n",
        "### Number of parameters:\n",
        "The number of parameters is given by:\n",
        "\n",
        "\n",
        "$$\\sum_{l=1}^{L} (d_l + 1) \\times d_{l+1}$$\n",
        "\n",
        "In the following, we consider functions with intermediate dimensions fixed to $m$, i.e., $d_1 = d_2 = \\dots = d_{L-1} = m$.\n",
        "\n",
        "We note:\n",
        "\n",
        "\n",
        "$$\\mathcal{N}_m = \\left\\{ \\hat{I}_m(x; \\theta): \\mathbb{R}^d \\rightarrow \\mathbb{R}, \\, \\theta \\in \\mathbb{R}^p \\right\\}$$\n",
        "\n",
        "\n",
        "$$\\mathcal{N}_m = \\left\\{ \\hat{I}_m : \\mathbb{R}^d \\rightarrow \\mathbb{R}, \\, \\theta \\in \\mathbb{R}^p \\right\\} \\quad \\text{with} \\quad p = m (d + 1) + m^2(L - 2)$$\n",
        "\n",
        "We also consider:\n",
        "\n",
        "\n",
        "$$\\mathcal{N}_{\\infty} = \\bigcup_m \\mathcal{N}_m$$\n",
        "\n",
        "---\n",
        "\n",
        "## Theorem: Universal Approximation in $L^2$\n",
        "\n",
        "Let $\\sigma$ be continuous and bounded (but not a ReLU function).  \n",
        "Let $\\mu$ be a probability measure on $\\mathbb{R}^d$.  \n",
        "Then, for all $L \\geq 2$, $\\mathcal{N}_{\\infty}$ is dense in $L^2(\\mathbb{R}^d, \\mu)$.\n",
        "\n",
        "---\n",
        "\n",
        "### Result (Hornik, 1991)\n",
        "\n",
        "### Application:\n",
        "Let $E[Z | X]$ be a random variable in $L^2(\\mathbb{R}^d, \\mu)$.  \n",
        "Then, we can approximate this random variable by an element of $\\mathcal{N}_m$ such that there is a sequence of elements of $\\mathcal{N}_m$ such that:\n",
        "\n",
        "\n",
        "$$\\lim_{m \\rightarrow \\infty} E\\left[ \\left( E[Z | X] - \\Phi_m(X; \\Theta_m) \\right)^2 \\right] = 0$$\n",
        "\n",
        "---\n",
        "\n",
        "### How to solve?\n",
        "\n",
        "We need to minimize $\\Theta^{(m)}$ such that:\n",
        "\n",
        "\n",
        "$$\\Theta^{(m)} = \\arg \\min_{\\Theta \\in \\mathbb{R}^p} E\\left[ \\left( Z_n^{(m)} - \\Phi_m(X_n; \\Theta) \\right)^2 \\right]$$\n",
        "\n",
        "As before (linear case), we look for a $\\Theta$ that cancels the gradient:\n",
        "\n",
        "\n",
        "$$\\nabla W(\\Theta) = -E\\left[ 2 \\nabla \\Phi_m(X_n, \\Theta) \\left( z_n^{(m)} - \\hat{I}_m(X_n; \\Theta) \\right) \\right]$$\n",
        "\n",
        "If the function can be differentiated, proceed using a gradient descent method.\n"
      ],
      "metadata": {
        "id": "h4-bBpIu1t3Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wzDL-UP2yukF"
      },
      "outputs": [],
      "source": [
        "## Definition of the previous function by using Pytorch\n",
        "def V_pytorch(x, n, models, N, K):\n",
        "    if n==N:\n",
        "        return torch.tensor(phi(x,K,n,N))\n",
        "    else :\n",
        "\n",
        "        return torch.tensor(np.maximum(phi(x,K,n,N),models[n].forward(torch.tensor(x, dtype=torch.float32).unsqueeze(1))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ojxH-3XCyM-z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "m = 4\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,m):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(1, m),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(m, m),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(m, m),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(m, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear_relu_stack(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "R_NcRhV4yM8-"
      },
      "outputs": [],
      "source": [
        "# Here we train our Neural Network bu using the loss :\n",
        "def nn_loss(model, S_t, K, n, N, models):\n",
        "    yy = V_pytorch(S_t[n+1], n+1, models, N, K)\n",
        "    S_n = torch.tensor(S_t[n], dtype=torch.float32).unsqueeze(1)\n",
        "    xx = model.forward(S_n)\n",
        "    loss = ((yy - xx) ** 2).mean()\n",
        "    return loss\n",
        "\n",
        "M_nn=5000\n",
        "\n",
        "def longstaff_Schwartz_nn(x0, K, T, N, M, m, num_epochs):\n",
        "\n",
        "    S_t = black_scholes_1d(n_times=N, n_paths=M, final_time= T, init_value=S0, r = r, sigma = sigma)\n",
        "    models = np.array([NeuralNetwork(m) for i in range(N)])\n",
        "    for i in reversed(range(N)):\n",
        "        model = NeuralNetwork(m)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "        losses = []\n",
        "        for epoch in range(num_epochs):\n",
        "            loss = nn_loss(model, S_t, K, i, N, models)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "            print(epoch)\n",
        "            print(loss)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        models[i] = model\n",
        "    price = max(phi(S0, K,0,N), models[0].forward(torch.tensor(S0, dtype=torch.float32).unsqueeze(-1)))\n",
        "    return price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxb2pGsZ7nbi",
        "outputId": "1b4bfe61-4645-4f3e-b192-bb6f78c22b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor(20.8576, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(20.6997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(20.5493, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(20.4059, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(20.2691, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(20.1385, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(20.0138, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(19.8947, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(19.7808, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(19.6718, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(19.5676, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(19.4678, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(19.3723, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(19.2808, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(19.1932, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(19.1093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(19.0289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(18.9518, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(18.8780, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(18.8072, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-eb8e585c9f6e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(np.maximum(phi(x,K,n,N),models[n].forward(torch.tensor(x, dtype=torch.float32).unsqueeze(1))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor(22.8172, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(63879.3607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(56.4432, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(46.2595, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(37.2283, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(29.3042, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(22.8185, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(18.1524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(15.3693, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(14.0523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(13.5628, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(13.4151, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(13.3768, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(13.3677, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(13.3657, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(13.3652, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(13.3651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(13.3651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(13.3651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(13.3651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(8.8017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(18.7118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(19.0647, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(22.8362, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(22.1416, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(21.4966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(20.8948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(20.3308, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(19.8005, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(19.3003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(18.8273, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(18.3791, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(17.9536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(17.5490, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(17.1637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(16.7964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(16.4460, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(16.1113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(15.7914, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(15.4855, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(13.6734, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(4109.4555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(32.7074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(31.7584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(30.8469, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(29.9695, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(29.1234, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(28.3061, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(27.5152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(26.7483, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(26.0035, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(25.2786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(24.5719, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(23.8813, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(23.2053, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(22.5422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(21.8905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(21.2490, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(20.6164, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(19.9919, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(90.4191, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(12.7050, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(12.5797, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(12.5019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(12.4538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(12.4242, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(12.4059, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(12.3947, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(12.3879, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(12.3837, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(12.3811, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(12.3796, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(12.3786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(12.3780, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(12.3777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(12.3775, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(12.3773, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(12.3772, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(12.3772, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(12.3772, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(9.0631, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(482.5724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(16.4878, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(15.8881, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(15.2966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(14.7131, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(14.1377, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(13.5709, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(13.0136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(12.4675, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(11.9344, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(11.4168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(10.9174, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(10.4393, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(9.9856, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(9.5595, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(9.1637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(8.8008, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(8.4725, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(8.1797, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(7.0803, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(49.8786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(19.1392, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(18.3497, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(17.6232, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(16.9492, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(16.3198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(15.7285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(15.1893, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(14.7120, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(14.2553, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(13.8173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(13.3964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(12.9913, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(12.6008, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(12.2239, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(11.8598, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(11.5078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(11.1673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(10.8377, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(10.6569, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(5.5646, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(5.7897, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(7.1223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(7.0365, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(10.8201, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(9.4754, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(6.6525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(6.6503, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(9.5731, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(7.2457, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(5.7928, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(6.6809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(6.1663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(7.9486, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(5.5427, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(5.5998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(5.8394, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(6.1208, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(7.6790, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(7.7110, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(18.0595, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(29.8098, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(11.4523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(10.2352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(9.7954, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(9.3841, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(8.9977, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(8.6335, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(8.2891, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(7.9625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(7.6521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(7.3564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(7.0744, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(6.8050, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(6.5474, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(6.3009, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(6.0650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(5.8391, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(5.6228, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "0\n",
            "tensor(14.2752, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "1\n",
            "tensor(4.0761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "2\n",
            "tensor(2.9462, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "3\n",
            "tensor(1.7715, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "4\n",
            "tensor(1.9321, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "5\n",
            "tensor(2.1255, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "6\n",
            "tensor(3.1105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "7\n",
            "tensor(1.7207, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "8\n",
            "tensor(1.7316, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "9\n",
            "tensor(1.7685, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "10\n",
            "tensor(1.8363, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "11\n",
            "tensor(2.1168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "12\n",
            "tensor(2.0567, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "13\n",
            "tensor(2.7618, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "14\n",
            "tensor(1.7322, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "15\n",
            "tensor(1.7617, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "16\n",
            "tensor(1.8013, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "17\n",
            "tensor(1.9599, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "18\n",
            "tensor(1.9540, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "19\n",
            "tensor(2.3844, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "tensor([2.0320])\n"
          ]
        }
      ],
      "source": [
        "price = longstaff_Schwartz_nn(S0, K, T, N, M_nn, 4, 20)\n",
        "print(price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8baCbjeq18OJ",
        "outputId": "f39f2a6c-3fed-47ba-c939-05ba0baf7696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price of an American Put with Neural Network 2.0320379734039307 $\n"
          ]
        }
      ],
      "source": [
        "print(\"The price of an American Put with Neural Network\", price[0].item(), \"$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhRqvFeN_X72"
      },
      "source": [
        "We observe that with only 5000 trajectories and 20 epochs we are closed to the theorical price. However this method is much more longer than the one using Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOhkBF4g3v-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}